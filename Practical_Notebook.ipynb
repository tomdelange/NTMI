{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tomdelange/NTMI/blob/main/Practical_Notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/IBM/LNN\n",
        "!pip install LTNtorch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bYiXrvFNOjIB",
        "outputId": "ac80f656-7e73-46b0-c985-a8965cc067d5"
      },
      "id": "bYiXrvFNOjIB",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/IBM/LNN\n",
            "  Cloning https://github.com/IBM/LNN to /tmp/pip-req-build-5o53wsox\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/IBM/LNN /tmp/pip-req-build-5o53wsox\n",
            "  Resolved https://github.com/IBM/LNN to commit 18ea03a52a79e6bbe8dada76e1ad9b320cd894d4\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting jupyter (from lnn==1.0)\n",
            "  Downloading jupyter-1.1.1-py2.py3-none-any.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: matplotlib>=3.3.3 in /usr/local/lib/python3.12/dist-packages (from lnn==1.0) (3.10.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from lnn==1.0) (3.6)\n",
            "Requirement already satisfied: numpy>=1.23.4 in /usr/local/lib/python3.12/dist-packages (from lnn==1.0) (2.0.2)\n",
            "Requirement already satisfied: pandas>=1.3.4 in /usr/local/lib/python3.12/dist-packages (from lnn==1.0) (2.2.2)\n",
            "Requirement already satisfied: setuptools>=65.5.1 in /usr/local/lib/python3.12/dist-packages (from lnn==1.0) (75.2.0)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.12/dist-packages (from lnn==1.0) (0.9.0)\n",
            "Requirement already satisfied: torch>=2.7.1 in /usr/local/lib/python3.12/dist-packages (from lnn==1.0) (2.9.0+cu126)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.12/dist-packages (from lnn==1.0) (4.67.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.3->lnn==1.0) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.3->lnn==1.0) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.3->lnn==1.0) (4.61.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.3->lnn==1.0) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.3->lnn==1.0) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.3->lnn==1.0) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.3->lnn==1.0) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.3->lnn==1.0) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.3.4->lnn==1.0) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.3.4->lnn==1.0) (2025.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=2.7.1->lnn==1.0) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.7.1->lnn==1.0) (4.15.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.7.1->lnn==1.0) (1.14.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.7.1->lnn==1.0) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2.7.1->lnn==1.0) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.7.1->lnn==1.0) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.7.1->lnn==1.0) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.7.1->lnn==1.0) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.7.1->lnn==1.0) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.7.1->lnn==1.0) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.7.1->lnn==1.0) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.7.1->lnn==1.0) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.7.1->lnn==1.0) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.7.1->lnn==1.0) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.7.1->lnn==1.0) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2.7.1->lnn==1.0) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=2.7.1->lnn==1.0) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.7.1->lnn==1.0) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.7.1->lnn==1.0) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.7.1->lnn==1.0) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.7.1->lnn==1.0) (3.5.0)\n",
            "Requirement already satisfied: notebook in /usr/local/lib/python3.12/dist-packages (from jupyter->lnn==1.0) (6.5.7)\n",
            "Requirement already satisfied: jupyter-console in /usr/local/lib/python3.12/dist-packages (from jupyter->lnn==1.0) (6.6.3)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.12/dist-packages (from jupyter->lnn==1.0) (7.16.6)\n",
            "Requirement already satisfied: ipykernel in /usr/local/lib/python3.12/dist-packages (from jupyter->lnn==1.0) (6.17.1)\n",
            "Requirement already satisfied: ipywidgets in /usr/local/lib/python3.12/dist-packages (from jupyter->lnn==1.0) (7.7.1)\n",
            "Collecting jupyterlab (from jupyter->lnn==1.0)\n",
            "  Downloading jupyterlab-4.5.0-py3-none-any.whl.metadata (16 kB)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.3->lnn==1.0) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.7.1->lnn==1.0) (1.3.0)\n",
            "Requirement already satisfied: debugpy>=1.0 in /usr/local/lib/python3.12/dist-packages (from ipykernel->jupyter->lnn==1.0) (1.8.15)\n",
            "Requirement already satisfied: ipython>=7.23.1 in /usr/local/lib/python3.12/dist-packages (from ipykernel->jupyter->lnn==1.0) (7.34.0)\n",
            "Requirement already satisfied: jupyter-client>=6.1.12 in /usr/local/lib/python3.12/dist-packages (from ipykernel->jupyter->lnn==1.0) (7.4.9)\n",
            "Requirement already satisfied: matplotlib-inline>=0.1 in /usr/local/lib/python3.12/dist-packages (from ipykernel->jupyter->lnn==1.0) (0.2.1)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.12/dist-packages (from ipykernel->jupyter->lnn==1.0) (1.6.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from ipykernel->jupyter->lnn==1.0) (5.9.5)\n",
            "Requirement already satisfied: pyzmq>=17 in /usr/local/lib/python3.12/dist-packages (from ipykernel->jupyter->lnn==1.0) (26.2.1)\n",
            "Requirement already satisfied: tornado>=6.1 in /usr/local/lib/python3.12/dist-packages (from ipykernel->jupyter->lnn==1.0) (6.5.1)\n",
            "Requirement already satisfied: traitlets>=5.1.0 in /usr/local/lib/python3.12/dist-packages (from ipykernel->jupyter->lnn==1.0) (5.7.1)\n",
            "Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.12/dist-packages (from ipywidgets->jupyter->lnn==1.0) (0.2.0)\n",
            "Requirement already satisfied: widgetsnbextension~=3.6.0 in /usr/local/lib/python3.12/dist-packages (from ipywidgets->jupyter->lnn==1.0) (3.6.10)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from ipywidgets->jupyter->lnn==1.0) (3.0.16)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.7.1->lnn==1.0) (3.0.3)\n",
            "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /usr/local/lib/python3.12/dist-packages (from jupyter-console->jupyter->lnn==1.0) (5.9.1)\n",
            "Requirement already satisfied: prompt-toolkit>=3.0.30 in /usr/local/lib/python3.12/dist-packages (from jupyter-console->jupyter->lnn==1.0) (3.0.52)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.12/dist-packages (from jupyter-console->jupyter->lnn==1.0) (2.19.2)\n",
            "Collecting async-lru>=1.0.0 (from jupyterlab->jupyter->lnn==1.0)\n",
            "  Downloading async_lru-2.0.5-py3-none-any.whl.metadata (4.5 kB)\n",
            "Requirement already satisfied: httpx<1,>=0.25.0 in /usr/local/lib/python3.12/dist-packages (from jupyterlab->jupyter->lnn==1.0) (0.28.1)\n",
            "Collecting jupyter-lsp>=2.0.0 (from jupyterlab->jupyter->lnn==1.0)\n",
            "  Downloading jupyter_lsp-2.3.0-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: jupyter-server<3,>=2.4.0 in /usr/local/lib/python3.12/dist-packages (from jupyterlab->jupyter->lnn==1.0) (2.14.0)\n",
            "Collecting jupyterlab-server<3,>=2.28.0 (from jupyterlab->jupyter->lnn==1.0)\n",
            "  Downloading jupyterlab_server-2.28.0-py3-none-any.whl.metadata (5.9 kB)\n",
            "Requirement already satisfied: notebook-shim>=0.2 in /usr/local/lib/python3.12/dist-packages (from jupyterlab->jupyter->lnn==1.0) (0.2.4)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.12/dist-packages (from nbconvert->jupyter->lnn==1.0) (4.13.5)\n",
            "Requirement already satisfied: bleach!=5.0.0 in /usr/local/lib/python3.12/dist-packages (from bleach[css]!=5.0.0->nbconvert->jupyter->lnn==1.0) (6.3.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.12/dist-packages (from nbconvert->jupyter->lnn==1.0) (0.7.1)\n",
            "Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.12/dist-packages (from nbconvert->jupyter->lnn==1.0) (0.3.0)\n",
            "Requirement already satisfied: mistune<4,>=2.0.3 in /usr/local/lib/python3.12/dist-packages (from nbconvert->jupyter->lnn==1.0) (3.1.4)\n",
            "Requirement already satisfied: nbclient>=0.5.0 in /usr/local/lib/python3.12/dist-packages (from nbconvert->jupyter->lnn==1.0) (0.10.2)\n",
            "Requirement already satisfied: nbformat>=5.7 in /usr/local/lib/python3.12/dist-packages (from nbconvert->jupyter->lnn==1.0) (5.10.4)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.12/dist-packages (from nbconvert->jupyter->lnn==1.0) (1.5.1)\n",
            "Requirement already satisfied: argon2-cffi in /usr/local/lib/python3.12/dist-packages (from notebook->jupyter->lnn==1.0) (25.1.0)\n",
            "Requirement already satisfied: Send2Trash>=1.8.0 in /usr/local/lib/python3.12/dist-packages (from notebook->jupyter->lnn==1.0) (1.8.3)\n",
            "Requirement already satisfied: terminado>=0.8.3 in /usr/local/lib/python3.12/dist-packages (from notebook->jupyter->lnn==1.0) (0.18.1)\n",
            "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.12/dist-packages (from notebook->jupyter->lnn==1.0) (0.23.1)\n",
            "Requirement already satisfied: nbclassic>=0.4.7 in /usr/local/lib/python3.12/dist-packages (from notebook->jupyter->lnn==1.0) (1.3.3)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.12/dist-packages (from bleach!=5.0.0->bleach[css]!=5.0.0->nbconvert->jupyter->lnn==1.0) (0.5.1)\n",
            "Requirement already satisfied: tinycss2<1.5,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from bleach[css]!=5.0.0->nbconvert->jupyter->lnn==1.0) (1.4.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.25.0->jupyterlab->jupyter->lnn==1.0) (4.12.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.25.0->jupyterlab->jupyter->lnn==1.0) (2025.11.12)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.25.0->jupyterlab->jupyter->lnn==1.0) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.25.0->jupyterlab->jupyter->lnn==1.0) (3.11)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.25.0->jupyterlab->jupyter->lnn==1.0) (0.16.0)\n",
            "Collecting jedi>=0.16 (from ipython>=7.23.1->ipykernel->jupyter->lnn==1.0)\n",
            "  Downloading jedi-0.19.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.12/dist-packages (from ipython>=7.23.1->ipykernel->jupyter->lnn==1.0) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.12/dist-packages (from ipython>=7.23.1->ipykernel->jupyter->lnn==1.0) (0.7.5)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.12/dist-packages (from ipython>=7.23.1->ipykernel->jupyter->lnn==1.0) (0.2.0)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.12/dist-packages (from ipython>=7.23.1->ipykernel->jupyter->lnn==1.0) (4.9.0)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.12/dist-packages (from jupyter-client>=6.1.12->ipykernel->jupyter->lnn==1.0) (0.4)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.12/dist-packages (from jupyter-core!=5.0.*,>=4.12->jupyter-console->jupyter->lnn==1.0) (4.5.1)\n",
            "Requirement already satisfied: jupyter-events>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter->lnn==1.0) (0.12.0)\n",
            "Requirement already satisfied: jupyter-server-terminals>=0.4.4 in /usr/local/lib/python3.12/dist-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter->lnn==1.0) (0.5.3)\n",
            "Requirement already satisfied: overrides>=5.0 in /usr/local/lib/python3.12/dist-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter->lnn==1.0) (7.7.0)\n",
            "Requirement already satisfied: websocket-client>=1.7 in /usr/local/lib/python3.12/dist-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter->lnn==1.0) (1.9.0)\n",
            "Requirement already satisfied: argon2-cffi-bindings in /usr/local/lib/python3.12/dist-packages (from argon2-cffi->notebook->jupyter->lnn==1.0) (25.1.0)\n",
            "Requirement already satisfied: babel>=2.10 in /usr/local/lib/python3.12/dist-packages (from jupyterlab-server<3,>=2.28.0->jupyterlab->jupyter->lnn==1.0) (2.17.0)\n",
            "Collecting json5>=0.9.0 (from jupyterlab-server<3,>=2.28.0->jupyterlab->jupyter->lnn==1.0)\n",
            "  Downloading json5-0.12.1-py3-none-any.whl.metadata (36 kB)\n",
            "Requirement already satisfied: jsonschema>=4.18.0 in /usr/local/lib/python3.12/dist-packages (from jupyterlab-server<3,>=2.28.0->jupyterlab->jupyter->lnn==1.0) (4.25.1)\n",
            "Requirement already satisfied: requests>=2.31 in /usr/local/lib/python3.12/dist-packages (from jupyterlab-server<3,>=2.28.0->jupyterlab->jupyter->lnn==1.0) (2.32.4)\n",
            "Requirement already satisfied: fastjsonschema>=2.15 in /usr/local/lib/python3.12/dist-packages (from nbformat>=5.7->nbconvert->jupyter->lnn==1.0) (2.21.2)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.12/dist-packages (from prompt-toolkit>=3.0.30->jupyter-console->jupyter->lnn==1.0) (0.2.14)\n",
            "Requirement already satisfied: ptyprocess in /usr/local/lib/python3.12/dist-packages (from terminado>=0.8.3->notebook->jupyter->lnn==1.0) (0.7.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4->nbconvert->jupyter->lnn==1.0) (2.8)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.12/dist-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel->jupyter->lnn==1.0) (0.8.5)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.28.0->jupyterlab->jupyter->lnn==1.0) (25.4.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.28.0->jupyterlab->jupyter->lnn==1.0) (2025.9.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.28.0->jupyterlab->jupyter->lnn==1.0) (0.37.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.28.0->jupyterlab->jupyter->lnn==1.0) (0.30.0)\n",
            "Requirement already satisfied: python-json-logger>=2.0.4 in /usr/local/lib/python3.12/dist-packages (from jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->lnn==1.0) (4.0.0)\n",
            "Requirement already satisfied: pyyaml>=5.3 in /usr/local/lib/python3.12/dist-packages (from jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->lnn==1.0) (6.0.3)\n",
            "Requirement already satisfied: rfc3339-validator in /usr/local/lib/python3.12/dist-packages (from jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->lnn==1.0) (0.1.4)\n",
            "Requirement already satisfied: rfc3986-validator>=0.1.1 in /usr/local/lib/python3.12/dist-packages (from jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->lnn==1.0) (0.1.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.31->jupyterlab-server<3,>=2.28.0->jupyterlab->jupyter->lnn==1.0) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.31->jupyterlab-server<3,>=2.28.0->jupyterlab->jupyter->lnn==1.0) (2.5.0)\n",
            "Requirement already satisfied: cffi>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from argon2-cffi-bindings->argon2-cffi->notebook->jupyter->lnn==1.0) (2.0.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->notebook->jupyter->lnn==1.0) (2.23)\n",
            "Requirement already satisfied: fqdn in /usr/local/lib/python3.12/dist-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->lnn==1.0) (1.5.1)\n",
            "Requirement already satisfied: isoduration in /usr/local/lib/python3.12/dist-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->lnn==1.0) (20.11.0)\n",
            "Requirement already satisfied: jsonpointer>1.13 in /usr/local/lib/python3.12/dist-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->lnn==1.0) (3.0.0)\n",
            "Requirement already satisfied: rfc3987-syntax>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->lnn==1.0) (1.1.0)\n",
            "Requirement already satisfied: uri-template in /usr/local/lib/python3.12/dist-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->lnn==1.0) (1.3.0)\n",
            "Requirement already satisfied: webcolors>=24.6.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->lnn==1.0) (25.10.0)\n",
            "Requirement already satisfied: lark>=1.2.2 in /usr/local/lib/python3.12/dist-packages (from rfc3987-syntax>=1.1.0->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->lnn==1.0) (1.3.1)\n",
            "Requirement already satisfied: arrow>=0.15.0 in /usr/local/lib/python3.12/dist-packages (from isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->lnn==1.0) (1.4.0)\n",
            "Downloading jupyter-1.1.1-py2.py3-none-any.whl (2.7 kB)\n",
            "Downloading jupyterlab-4.5.0-py3-none-any.whl (12.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.4/12.4 MB\u001b[0m \u001b[31m93.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading async_lru-2.0.5-py3-none-any.whl (6.1 kB)\n",
            "Downloading jupyter_lsp-2.3.0-py3-none-any.whl (76 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.7/76.7 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jupyterlab_server-2.28.0-py3-none-any.whl (59 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.8/59.8 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jedi-0.19.2-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m58.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading json5-0.12.1-py3-none-any.whl (36 kB)\n",
            "Building wheels for collected packages: lnn\n",
            "  Building wheel for lnn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for lnn: filename=lnn-1.0-py3-none-any.whl size=80076 sha256=8e6132e5540a63f9d109ed376bd480c1b38bf7a8a01460157adec205525dd884\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-8vfgade8/wheels/09/02/50/7a4831741be04697889ef7c2ca762bca7658c30988246851df\n",
            "Successfully built lnn\n",
            "Installing collected packages: json5, jedi, async-lru, jupyterlab-server, jupyter-lsp, jupyterlab, jupyter, lnn\n",
            "Successfully installed async-lru-2.0.5 jedi-0.19.2 json5-0.12.1 jupyter-1.1.1 jupyter-lsp-2.3.0 jupyterlab-4.5.0 jupyterlab-server-2.28.0 lnn-1.0\n",
            "Collecting LTNtorch\n",
            "  Downloading LTNtorch-1.0.2-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from LTNtorch) (2.0.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from LTNtorch) (2.9.0+cu126)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch->LTNtorch) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch->LTNtorch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch->LTNtorch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->LTNtorch) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch->LTNtorch) (3.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->LTNtorch) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch->LTNtorch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->LTNtorch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->LTNtorch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch->LTNtorch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch->LTNtorch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch->LTNtorch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch->LTNtorch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch->LTNtorch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch->LTNtorch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch->LTNtorch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch->LTNtorch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch->LTNtorch) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch->LTNtorch) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->LTNtorch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch->LTNtorch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch->LTNtorch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch->LTNtorch) (3.5.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->LTNtorch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->LTNtorch) (3.0.3)\n",
            "Downloading LTNtorch-1.0.2-py3-none-any.whl (29 kB)\n",
            "Installing collected packages: LTNtorch\n",
            "Successfully installed LTNtorch-1.0.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "19197c84",
      "metadata": {
        "id": "19197c84"
      },
      "source": [
        "# Practical Worksheet\n",
        "\n",
        "In this worksheet, we will be working with a small dataset of hyponym-hypernym pairs. Hyponymy is the `is-a` relation. So we will have pairs like `(cat, mammal)` meaning 'A cat is a mammal'. The hyponym is the more specific term (e.g., cat) and the hypernym is the more general term (e.g., mammal). In this notebook you will:\n",
        "\n",
        "1. (3 pts) Use Logical Neural Networks with a very small hyponym dataset to infer a set of facts. You will discuss the kinds of facts that you can infer and the limitations of the model as it is implemented\n",
        "2. (5 pts) Set up a Logic Tensor Network to learn word embeddings and predicates that can model a larger hyponymy dataset.\n",
        "3. (5 pts) Evaluate the effect of different axioms in the LTN system.\n",
        "4. (2 pts) Query your model.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2b15d0af",
      "metadata": {
        "id": "2b15d0af"
      },
      "source": [
        "## Part 0. Setup\n",
        "Create an environment and install python 3.12, numpy, pandas, and scikit-learn.\n",
        "\n",
        "Install LNNs using `pip install git+https://github.com/IBM/LNN`\n",
        "\n",
        "Install LTNs using `pip install LTNtorch`\n",
        "\n",
        "Import packages as below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "03699930",
      "metadata": {
        "id": "03699930"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import numpy as np\n",
        "import ltn"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "84644f44",
      "metadata": {
        "id": "84644f44"
      },
      "source": [
        "## Part 1. Inferring facts using Logical Neural Networks\n",
        "\n",
        "In this first part, we will manually specify a very small dictionary of hyponym facts. We have three hyponyms and three non-hyponyms. The hyponymy relation is transitive, meaning that if $x$ is a hyponym of $y$ and $y$ is a hyponym of $z$, then $x$ should be a hyponym of $z$.\n",
        "\n",
        "You will:\n",
        "\n",
        "a. (1.5 pt) Set up a LNN model with suitable variables, a transitivity axiom, and hyponymy data.\n",
        "\n",
        "b. (0.5 pt) Run inference over the model.\n",
        "\n",
        "c. (1 pt) Inspect the output of the model and discuss whether the output is as expected."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "032bfc1c",
      "metadata": {
        "id": "032bfc1c"
      },
      "outputs": [],
      "source": [
        "# We first set up a small dictionary of hyponyms\n",
        "from lnn import Fact\n",
        "\n",
        "hyp_dict = {('cat', 'mammal'):Fact.TRUE,\n",
        "            ('dog', 'mammal'):Fact.TRUE,\n",
        "            ('mammal', 'animal'):Fact.TRUE,\n",
        "            ('cat', 'dog'):Fact.FALSE,\n",
        "            ('animal', 'mammal'):Fact.FALSE,\n",
        "            ('mammal', 'dog'):Fact.FALSE,}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dbafd55d",
      "metadata": {
        "id": "dbafd55d"
      },
      "source": [
        "### Part 1a) (1.5 pts) Setting up the model.\n",
        "Set up a LNN model with suitable predicates and variables, a transitivity axiom, and hyponymy data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6a13684b",
      "metadata": {
        "id": "6a13684b"
      },
      "outputs": [],
      "source": [
        "# Initialize an empty model\n",
        "from lnn import Model\n",
        "model = Model()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1697655b",
      "metadata": {
        "id": "1697655b"
      },
      "outputs": [],
      "source": [
        "from lnn import Predicates, Variables\n",
        "# Create a predicate of arity 2 called Hyps and three variables x, y, z\n",
        "Hyps = Predicates(\"Hyps\", arity=2)\n",
        "x, y, z = Variables(\"x\", \"y\", \"z\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f9d25b81",
      "metadata": {
        "id": "f9d25b81"
      },
      "outputs": [],
      "source": [
        "from lnn import Implies, And\n",
        "# Create a logical rule that encodes the fact that the hyponymy relation is transitive\n",
        "transitive = Implies(And(Hyps(x, y), Hyps(y, z)), Hyps(x, z))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cfc34a56",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 315
        },
        "id": "cfc34a56",
        "outputId": "1d1a6ba2-eb79-42b3-8e3e-be8a73e22110"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "formula expected of type Formula, received tuple",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-408917117.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Add the knowledge and the data (the hyponymy dict) to the model and print.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_knowledge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransitive\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhyp_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/lnn/model.py\u001b[0m in \u001b[0;36madd_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    343\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mformula\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfact\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mformula\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFormula\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m                 raise TypeError(\n\u001b[0m\u001b[1;32m    346\u001b[0m                     \u001b[0;34m\"formula expected of type Formula, received \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m                     \u001b[0;34mf\"{formula.__class__.__name__}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: formula expected of type Formula, received tuple"
          ]
        }
      ],
      "source": [
        "# Add the knowledge and the data (the hyponymy dict) to the model and print.\n",
        "model.add_knowledge(transitive)\n",
        "model.add_data(hyp_dict)\n",
        "model.print()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2c42cce3",
      "metadata": {
        "id": "2c42cce3"
      },
      "source": [
        "### Part 1b) (0.5 pts) Inferring facts\n",
        "Run inference over the model and print the output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "58bf7202",
      "metadata": {
        "id": "58bf7202"
      },
      "outputs": [],
      "source": [
        "# Part 1b (0.5 pts) Run inference over the model and print the output\n",
        "## YOUR CODE HERE ##"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a0a0f233",
      "metadata": {
        "id": "a0a0f233"
      },
      "source": [
        "### Part 1c) (1 pt) Inspecting the output.\n",
        "\n",
        "You should see that there are various facts whose truth value is unknown.\n",
        "\n",
        "Q1: Why can we not infer the truth value of all facts with the given database and axioms?\n",
        "\n",
        "Q2: Suggest a suitable axiom to add to this system that would help to infer more facts. You do not need to implement the axiom."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "717ff74d",
      "metadata": {
        "id": "717ff74d"
      },
      "source": [
        "YOUR ANSWER HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e0b131b6",
      "metadata": {
        "id": "e0b131b6"
      },
      "source": [
        "## Part 2 (5 pts) Building Embeddings with Logic Tensor Networks.\n",
        "In this part, we will build a Logic Tensor Network to learn embeddings for the hyponyms. You will:\n",
        "\n",
        "a. (1 pt) Describe why learning embeddings for the hyponyms is a suitable approach.\n",
        "\n",
        "b. (1 pt) Set up a predicate for the hyponymy relation.\n",
        "\n",
        "c. (1 pt) Train a simple network on the hyponymy task.\n",
        "\n",
        "d. (2 pts) Assess satisfaction on the test set  and negative sample set\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "edd42761",
      "metadata": {
        "id": "edd42761"
      },
      "source": [
        "### Importing the data\n",
        "\n",
        "Below, we import the data into pandas dataframes. Take a look at the data to familiarise yourself with the format. In each .csv file we have a list of word pairs.\n",
        "- In train_hypernyms we have the set of hypernym pairs we will train on.\n",
        "- In test_hypernyms we have the set of pairs we will test on.\n",
        "- In non_hypernyms we have a set of word pairs that are not hypernym pairs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "36661dfc",
      "metadata": {
        "id": "36661dfc"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "train_df = pd.read_csv('train_hypernyms.csv')\n",
        "test_df = pd.read_csv('test_hypernyms.csv')\n",
        "neg_df = pd.read_csv('non_hypernyms.csv')\n",
        "\n",
        "\n",
        "train_pairs = train_df.values\n",
        "test_pairs = test_df.values\n",
        "neg_pairs = neg_df.values\n",
        "\n",
        "print(\"Training pairs:\")\n",
        "print(train_pairs[:5])\n",
        "\n",
        "print(\"Testing pairs:\")\n",
        "print(test_pairs[:5])\n",
        "\n",
        "print(\"Negative pairs:\")\n",
        "print(neg_pairs[:5])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "445ad7aa",
      "metadata": {
        "id": "445ad7aa"
      },
      "source": [
        "### Part 2a. (1 pt) Learning Embeddings\n",
        "\n",
        "When we use a logic tensor network, we can choose to use data from outside sources or to train embeddings within the network. We will be training embeddings. Do you think this is a suitable approach for this dataset? Why or why not?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6d647aef",
      "metadata": {
        "id": "6d647aef"
      },
      "source": [
        "YOUR ANSWER HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "884e33e8",
      "metadata": {
        "id": "884e33e8"
      },
      "source": [
        "Below, we will set up the vocabulary and the initial random word embeddings to be trained."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "da40ce30",
      "metadata": {
        "id": "da40ce30"
      },
      "outputs": [],
      "source": [
        "# Build a set of vocab by taking the union of the hyponyms and hypernyms\n",
        "vocab = set(train_df.hyper.unique()).union(train_df.hypo.unique())\n",
        "\n",
        "# Set the dimension of the vocab to 10\n",
        "vocab_dim = 10\n",
        "\n",
        "# Build a dictionary of word embeddings initialised randomly and set to be trainable.\n",
        "word_embeddings = {word: ltn.Constant(torch.rand((vocab_dim,)), trainable=True) \\\n",
        "                   for word in vocab}\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1f706ae5",
      "metadata": {
        "id": "1f706ae5"
      },
      "source": [
        "### Part 2b. (1 pt) Defining a predicate.\n",
        "Define a predicate as a feed-forward NN with ELU and sigmoid activation functions and one hidden layer of size 16"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "71c8835c",
      "metadata": {
        "id": "71c8835c"
      },
      "outputs": [],
      "source": [
        "# Define a feed-forward NN  with ELU and sigmoid activation functions and one hidden layer of size 16.\n",
        "class ModelHyp(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        ## YOUR CODE HERE ##\n",
        "\n",
        "\n",
        "    def forward(self, *x):\n",
        "        # Specify the forward pass with ELU on the hidden layers and sigmoid on the output\n",
        "        x = list(x)\n",
        "        x = torch.cat(x, dim=1)\n",
        "        ## YOUR CODE HERE ##\n",
        "\n",
        "# Wrap the feed-forward NN to make it an LTN predicate called Hyp\n",
        "Hyp = ## YOUR CODE HERE ##\n",
        "\n",
        "# Define connectives, quantifiers, and SatAgg\n",
        "And = ltn.Connective(ltn.fuzzy_ops.AndProd())\n",
        "Not = ltn.Connective(ltn.fuzzy_ops.NotStandard())\n",
        "Implies = ltn.Connective(ltn.fuzzy_ops.ImpliesReichenbach())\n",
        "Forall = ltn.Quantifier(ltn.fuzzy_ops.AggregPMeanError(p=2), quantifier=\"f\")\n",
        "SatAgg = ltn.fuzzy_ops.SatAgg()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5e276e36",
      "metadata": {
        "id": "5e276e36"
      },
      "source": [
        "### Part 2c. (1 pt) Training the network\n",
        "\n",
        "We set up a simple network in which we view our knowledge base as consisting just of those pairs in the training set. So our knowledge base states that for each word pair in the training set, this is a hyponym pair. We want to maximise the satisfaction over this knowledge base. To do this, we write a suitable axiom to aggregate the satisfaction of the hyponymy predicate over these pairs, and train the parameters of the network."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "22e5c413",
      "metadata": {
        "id": "22e5c413"
      },
      "outputs": [],
      "source": [
        "# We have to optimize the parameters of the predicate and also of the embeddings\n",
        "params = list(Hyp.parameters()) +[i.value for i in word_embeddings.values()]\n",
        "optimizer = torch.optim.Adam(params, lr=0.001)\n",
        "\n",
        "# Set up a training loop for 300 epochs\n",
        "for epoch in range(300):\n",
        "    # Set up a variable sat_agg which is the result of aggregating the truth values of all the axioms\n",
        "    sat_agg = SatAgg(\n",
        "# Implement one axiom which aggregates the satisfaction across the (x, y) in train_pairs\n",
        "        ## YOUR CODE HERE ##\n",
        "        # Our list of hyponym pairs is in train_pairs.\n",
        "        # We want to maximise the satisfaction gained by inputting the embeddings of those words into\n",
        "        # our hyponymy predicate.\n",
        "\n",
        "\n",
        "    )\n",
        "\n",
        "    loss = 1. - sat_agg\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # Print metrics every 20 epochs of training\n",
        "    if epoch % 20 == 0:\n",
        "        print(f\" epoch {epoch} | loss {loss} | Train Sat {sat_agg}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "41705558",
      "metadata": {
        "id": "41705558"
      },
      "source": [
        "### Part 2d (2 pts) Assessing the satisfaction on the test set\n",
        "\n",
        "Calculate the satisfaction over the test set using SatAgg. Do you think the model is generalising well? Now calculate the satisfaction over the negative samples dataset. Is this a suitable satisfaction level? Why or why not?\n",
        "\n",
        "YOUR ANSWER HERE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7c63e4d8",
      "metadata": {
        "id": "7c63e4d8"
      },
      "outputs": [],
      "source": [
        "print(f\"the satisfaction of the test dataset is: ## YOUR CODE HERE ##\")\n",
        "\n",
        "print(f\"the satisfaction of the negative dataset is: ## YOUR CODE HERE ##\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "750b0390",
      "metadata": {
        "id": "750b0390"
      },
      "source": [
        "## Part 3. (5 pts) Evaluate the effect of different axioms in the LTN system\n",
        "\n",
        "In this part you will:\n",
        "\n",
        "a. (2 pts) Retrain the model and evaluate the performance with negation included\n",
        "\n",
        "b. (2 pts) Retrain the model and evaluate performance with transitivity included\n",
        "\n",
        "c. (1 pt) Discuss the effect of the different axioms introduced."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b3aaad1a",
      "metadata": {
        "id": "b3aaad1a"
      },
      "source": [
        "### Part 3a. (2pts)  Retraining the model with negation\n",
        "Reinitialise the model and retrain, including information from the `neg_pairs` dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ac747fbf",
      "metadata": {
        "id": "ac747fbf"
      },
      "outputs": [],
      "source": [
        "# Reinitialise the model\n",
        "Hyp = ## YOUR CODE HERE ##"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "473942d0",
      "metadata": {
        "id": "473942d0"
      },
      "outputs": [],
      "source": [
        "# Set up the parameters and optimizer\n",
        "## YOUR CODE HERE ##\n",
        "\n",
        "# Set up a training loop for 300 epochs\n",
        "    ## YOUR CODE HERE ##\n",
        "    # Set up a variable sat_agg which is the result of aggregating the truth values of all the axioms\n",
        "        ## YOUR CODE HERE ##\n",
        "        # Implement one axiom which aggregates the satisfaction across the (x, y) in train_pairs\n",
        "        ## YOUR CODE HERE ##\n",
        "\n",
        "        # Implement one axiom which aggregates the satisfaction across the (x, y) in neg_pairs\n",
        "        # Note that this statement should involve a negation.\n",
        "        ## YOUR CODE HERE ##\n",
        "\n",
        "\n",
        "    # Calculate the loss and propagate backwards\n",
        "    ## YOUR CODE HERE ##\n",
        "\n",
        "    # Print metrics every 20 epochs of training\n",
        "    ## YOUR CODE HERE ##"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "768815fe",
      "metadata": {
        "id": "768815fe"
      },
      "outputs": [],
      "source": [
        "# Calculate the satisfaction across the test dataset and the negated dataset\n",
        "print(f\"the satisfaction of the test dataset is: ## YOUR CODE HERE ##\")\n",
        "\n",
        "print(f\"the satisfaction of the negative dataset is: ## YOUR CODE HERE ##\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cfdbc5e2",
      "metadata": {
        "id": "cfdbc5e2"
      },
      "source": [
        "### Part 3b. (2 pts) Retraining the model with transitivity\n",
        "\n",
        "As we discussed in Part 1, the hyponymy relation is transitive. This should be reflected in the axioms. Reinitialise the model and add an axiom expressing the rule:\n",
        "\n",
        "$\\forall x, y, z Hyp(x, y) \\land Hyp(y, z) \\implies Hyp(x, z)$\n",
        "\n",
        "Retrain the model and evaluate on the test and negated datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "de90872a",
      "metadata": {
        "id": "de90872a"
      },
      "outputs": [],
      "source": [
        "# Reinitialise the model\n",
        "## YOUR CODE HERE ##"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ce13b58d",
      "metadata": {
        "id": "ce13b58d"
      },
      "outputs": [],
      "source": [
        "# Set up the parameters and optimizer\n",
        "## YOUR CODE HERE ##\n",
        "\n",
        "# Set up a training loop for 300 epochs\n",
        "## YOUR CODE HERE ##\n",
        "\n",
        "    # Create variables x_, y_, and z_, grounded with values from the `word_embeddings` dictionary\n",
        "    ## YOUR CODE HERE ##\n",
        "\n",
        "    # Set up a variable sat_agg which is the result of aggregating the truth values of all the axioms\n",
        "    ## YOUR CODE HERE ##\n",
        "\n",
        "        #Positive instances of hyponymy\n",
        "        ## YOUR CODE HERE ##\n",
        "\n",
        "        #Negative instances of hyponymy\n",
        "        ## YOUR CODE HERE ##\n",
        "\n",
        "        # Transitivity axiom\n",
        "        ## YOUR CODE HERE ##\n",
        "\n",
        "\n",
        "    # Calculate the loss and propagate backwards\n",
        "    ## YOUR CODE HERE ##\n",
        "\n",
        "    # Print metrics every 20 epochs of training\n",
        "    ## YOUR CODE HERE ##"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ebcefe52",
      "metadata": {
        "id": "ebcefe52"
      },
      "outputs": [],
      "source": [
        "# Calculate the satisfaction across the test dataset and the negated dataset\n",
        "print(f\"the satisfaction of the test dataset is: ## YOUR CODE HERE ##\")\n",
        "\n",
        "print(f\"the satisfaction of the negative dataset is: ## YOUR CODE HERE ##\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5c7d1248",
      "metadata": {
        "id": "5c7d1248"
      },
      "source": [
        "### Part 3c. (1 pt)  Evaluating the model\n",
        "How has the satisfaction changed across the test set and the set of negative examples as you include different axioms? Why has this happened? Write a couple of sentences with your conclusions about the datasets and the model you have built."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e8ed9e73",
      "metadata": {
        "id": "e8ed9e73"
      },
      "source": [
        "YOUR ANSWER HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7ded3b40",
      "metadata": {
        "id": "7ded3b40"
      },
      "source": [
        "## Part 4 (2 pts) Querying the model\n",
        "\n",
        "One of the strengths of Logic Tensor Networks is that you are able to query the models you have built. In this part you will:\n",
        "\n",
        "a. (0.5 pts) Define a logical statement that you expect to hold in your model.\n",
        "\n",
        "b. (1 pt) Query the model.\n",
        "\n",
        "c. (0.5 pts) Discuss your result."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "42a26fbd",
      "metadata": {
        "id": "42a26fbd"
      },
      "source": [
        "### Part 4a. (0.5 pts) Defining a query\n",
        "\n",
        "Thinking about the properties of hyponymy, give a logical statement that you would expect to hold in your model. The statement can be quite simple."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c11a874b",
      "metadata": {
        "id": "c11a874b"
      },
      "source": [
        "YOUR ANSWER HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4b53450e",
      "metadata": {
        "id": "4b53450e"
      },
      "source": [
        "### Part 4b. (1 pt) Querying the model\n",
        "\n",
        "Write a function that returns the satisfaction level of your logical statement and determine the satisfaction level."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "daddef07",
      "metadata": {
        "id": "daddef07"
      },
      "outputs": [],
      "source": [
        "# this function returns the satisfaction level of your logical formula\n",
        "def phi():\n",
        "    # Create variables p, q, and r and initialize with the values from 'word_embeddings'\n",
        "    ## YOUR CODE HERE ##\n",
        "    # Return the truth value of phi\n",
        "    ## YOUR CODE HERE ##\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d394cb31",
      "metadata": {
        "id": "d394cb31"
      },
      "outputs": [],
      "source": [
        "# Evaluate phi\n",
        "## YOUR CODE HERE ##"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d02ffc37",
      "metadata": {
        "id": "d02ffc37"
      },
      "source": [
        "### Part 4c. (0.5 pts) Discuss the results\n",
        "\n",
        "Was the satisfaction value what you expected to see? Why or why not?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "78f5608b",
      "metadata": {
        "id": "78f5608b"
      },
      "source": [
        "YOUR ANSWER HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "22ba5b2a",
      "metadata": {
        "id": "22ba5b2a"
      },
      "source": [
        "## Wrap up\n",
        "\n",
        "In this worksheet, we looked at the hyponymy relation that can hold between words.\n",
        "\n",
        "1. We used Logical Neural Networks with a very small hyponym dataset to infer a set of facts, and discussed the kinds of facts that you can infer and the limitations of the model as it is implemented.\n",
        "2. We set up a Logic Tensor Network to learn word embeddings and predicates that can model a larger hyponymy dataset.\n",
        "3. We evaluated the effect of different axioms in the LTN system.\n",
        "4. And finally, you queried your model with new logical statements.\n",
        "\n",
        "For another 15 points, you can extend this worksheet in a number of different ways.\n",
        "\n",
        "### Possible extensions\n",
        "\n",
        "1. Use a new dataset for the task of inferring relationships over data.\n",
        "2. Use the same dataset with a different model that we have covered in class. You could potentially use Logical Neural Networks, although they are a little slow.\n",
        "3. Extend the investigation already started in this notebook. How do you expect the hyponymy relation to behave? Can you improve performance on novel queries?\n",
        "4. Extend this investigation by including semantic information into the word embeddings from external sources.\n",
        "5. Other ideas? Feel free to discuss with me!\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c8b2fbb1",
      "metadata": {
        "id": "c8b2fbb1"
      },
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "nesy_practical",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}